# ğŸŒ³ Random Forest

## ğŸ” Ce este Random Forest?

Random Forest este un algoritm de **Ã®nvÄƒÈ›are supravegheatÄƒ** bazat pe **ensemble learning**, care creeazÄƒ o â€pÄƒdureâ€ de **mai mulÈ›i arbori de decizie** (decision trees) È™i ia decizia finalÄƒ pe baza **votului majoritar** (pentru clasificare) sau **mediei** (pentru regresie).

---

## âš™ï¸ Cum funcÈ›ioneazÄƒ?

1. Se creeazÄƒ mai mulÈ›i arbori de decizie, fiecare antrenat pe un subset diferit al datelor (prin **bootstrap sampling**).
2. La fiecare split al unui arbore, se selecteazÄƒ aleatoriu doar un subset de caracteristici.
3. Se face predicÈ›ia:
   - **Clasificare**: votul majoritar al arborilor.
   - **Regresie**: media predicÈ›iilor arborilor.

---

## ğŸ”§ Parametri importanÈ›i

| Parametru          | Descriere |
|--------------------|-----------|
| `n_estimators`     | NumÄƒrul de arbori din pÄƒdure. Valori mari â†’ stabilitate, dar cost computaÈ›ional mai mare. |
| `max_depth`        | AdÃ¢ncimea maximÄƒ a fiecÄƒrui arbore. EvitÄƒ overfitting-ul. |
| `max_features`     | NumÄƒrul maxim de caracteristici considerate la fiecare split. |
| `bootstrap`        | DacÄƒ sÄƒ foloseascÄƒ bootstrap sampling (True/False). |
| `random_state`     | Pentru reproducibilitate. |
| `criterion`        | FuncÈ›ia de impureÈ›e (`gini`, `entropy` pentru clasificare, `mse`, `mae` pentru regresie). |

---

## âœ… Avantaje

- Robust la overfitting.
- Poate gestiona date lipsÄƒ È™i categorii.
- BunÄƒ performanÈ›Äƒ pe date complexe.
- PoÈ›i obÈ›ine **importanÈ›a caracteristicilor**.

---

## âŒ Dezavantaje

- Mai lent decÃ¢t un singur arbore de decizie.
- Mai greu de interpretat.
- Nu e ideal pentru date extrem de mari fÄƒrÄƒ optimizÄƒri.

---

## ğŸ§  CÃ¢nd sÄƒ foloseÈ™ti Random Forest?

- CÃ¢nd vrei un model solid, cu performanÈ›Äƒ bunÄƒ, fÄƒrÄƒ multÄƒ reglare.
- CÃ¢nd ai multe caracteristici È™i/sau date cu zgomot.
- Pentru probleme de clasificare È™i regresie generalÄƒ.
# âš¡ XGBoost (Extreme Gradient Boosting)

## ğŸ” Ce este XGBoost?

XGBoost este un algoritm de boosting foarte performant, bazat pe arbori de decizie. Este o **tehnicÄƒ de ensemble learning**, care creeazÄƒ arbori secvenÈ›ial, fiecare Ã®ncercÃ¢nd sÄƒ **corecteze greÈ™elile** anterioare.

---

## ğŸš€ De ce este â€extremeâ€?

- Optimizare avansatÄƒ a vitezei È™i memoriei.
- Regularizare `L1` È™i `L2` pentru prevenirea overfitting-ului.
- Suport paralelizat È™i scalabil.
- Foarte folosit Ã®n competiÈ›ii (ex: Kaggle).

---

## âš™ï¸ Cum funcÈ›ioneazÄƒ?

1. Se porneÈ™te de la un model slab (de obicei un arbore mic).
2. Fiecare arbore nou este construit pentru a reduce erorile modelului anterior.
3. Se foloseÈ™te **Gradient Boosting** pentru actualizarea greutÄƒÈ›ilor.

---

## ğŸ”§ Parametri importanÈ›i

| Parametru          | Descriere |
|--------------------|-----------|
| `n_estimators`     | NumÄƒrul total de arbori (iteraÈ›ii). |
| `learning_rate`    | CÃ¢t de mult influenÈ›eazÄƒ fiecare arbore nou modelul final. Valori mai mici â†’ Ã®nvÄƒÈ›are mai lentÄƒ, dar mai stabilÄƒ. |
| `max_depth`        | AdÃ¢ncimea maximÄƒ a fiecÄƒrui arbore. |
| `subsample`        | Procentul de date folosite pentru fiecare arbore. AjutÄƒ la regularizare. |
| `colsample_bytree` | Procentul de caracteristici selectate pentru fiecare arbore. |
| `gamma`            | Minimul necesar de reducere a pierderii pentru a face un split. |
| `reg_alpha`, `reg_lambda` | Termeni de regularizare L1 È™i L2. |

---

## âœ… Avantaje

- Extrem de precis.
- Scalabil È™i rapid.
- Previne overfitting-ul.
- AcceptÄƒ date lipsÄƒ È™i categorice (Ã®n unele implementÄƒri).

---

## âŒ Dezavantaje

- Mai greu de Ã®nÈ›eles È™i ajustat.
- NecesitÄƒ reglaj de parametri (`grid search`, `optuna` etc.).
- Nu e ideal pentru date extrem de zgomotoase fÄƒrÄƒ filtrare.

---

## ğŸ§  CÃ¢nd sÄƒ foloseÈ™ti XGBoost?

- Pentru competiÈ›ii È™i aplicaÈ›ii unde performanÈ›a e criticÄƒ.
- Pentru seturi de date tabulare, mari È™i complexe.
- CÃ¢nd ai timp È™i resurse pentru tuning.

---


