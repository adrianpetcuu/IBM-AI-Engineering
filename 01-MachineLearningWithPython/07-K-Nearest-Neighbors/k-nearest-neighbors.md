# ğŸ¤– K Nearest Neighbors (KNN)

## ğŸ” Ce este KNN?

KNN este un algoritm de Ã®nvÄƒÈ›are supravegheatÄƒ (supervised learning) folosit pentru **clasificare** È™i **regresie**.  
Este un **algoritm bazat pe instanÈ›e (lazy learning)** â€“ nu Ã®nvaÈ›Äƒ un model Ã®n prealabil, ci face predicÈ›ii folosind **vecinii cei mai apropiaÈ›i** din setul de antrenament.

---

## âš™ï¸ Cum funcÈ›ioneazÄƒ?

1. Se alege un numÄƒr `K` (numÄƒr de vecini).
2. Pentru o nouÄƒ observaÈ›ie, se calculeazÄƒ distanÈ›a faÈ›Äƒ de toate datele din setul de antrenament.
3. Se selecteazÄƒ cei mai apropiaÈ›i `K` vecini.
4. Se face predicÈ›ia:
   - **Clasificare**: se alege clasa cea mai frecventÄƒ dintre vecini.
   - **Regresie**: se face media valorilor vecinilor.

---

## ğŸ“ Metode de calcul al distanÈ›ei

- **DistanÈ›a EuclidianÄƒ** (cel mai frecvent):
  \[
  d(p, q) = \sqrt{ \sum_{i=1}^{n} (p_i - q_i)^2 }
  \]
- DistanÈ›a Manhattan
- DistanÈ›a Minkowski
- DistanÈ›Äƒ Cosinus, etc.

---

## ğŸ”§ Parametri importanÈ›i Ã®n KNN

| Parametru | Descriere |
|-----------|-----------|
| `n_neighbors` | NumÄƒrul de vecini `K`. Trebuie ajustat cu grijÄƒ â€“ prea mic â†’ overfitting, prea mare â†’ underfitting. |
| `weights`     | Modul de ponderare a vecinilor: `'uniform'` (toÈ›i egal) sau `'distance'` (vecinii apropiaÈ›i au mai multÄƒ influenÈ›Äƒ). |
| `metric`      | MÄƒsura de distanÈ›Äƒ folositÄƒ (`'euclidean'`, `'manhattan'`, `'minkowski'` etc.). |
| `algorithm`   | Algoritm de cÄƒutare: `'auto'`, `'ball_tree'`, `'kd_tree'`, `'brute'`. Poate influenÈ›a viteza de execuÈ›ie. |

---

## âœ… Avantaje

- UÈ™or de Ã®nÈ›eles È™i implementat.
- FÄƒrÄƒ antrenare propriu-zisÄƒ (lazy learner).
- FuncÈ›ioneazÄƒ bine pentru seturi de date mici È™i bine etichetate.

---

## âŒ Dezavantaje

- Devine lent pentru seturi de date mari.
- Sensibil la **scalarea datelor** È™i la **dimensionalitate mare**.
- PerformanÈ›Äƒ slabÄƒ Ã®n caz de **date zgomotoase** sau dezechilibrate.

---

## ğŸ¯ CÃ¢nd sÄƒ foloseÈ™ti KNN?

- Pentru **probleme de clasificare** cu puÈ›ine caracteristici.
- CÃ¢nd interpretabilitatea modelului este importantÄƒ.
- CÃ¢nd ai date puÈ›ine È™i nu vrei antrenare lungÄƒ.

---

## ğŸ§ª RecomandÄƒri practice

- FoloseÈ™te **StandardScaler** sau **MinMaxScaler** Ã®nainte de a aplica KNN (pentru cÄƒ se bazeazÄƒ pe distanÈ›e).
- TesteazÄƒ mai multe valori ale lui `K` (de ex. `1, 3, 5, 7, 11`) folosind **cross-validation**.
- EvitÄƒ KNN pe seturi de date foarte mari fÄƒrÄƒ optimizÄƒri (ex: folosirea de KD-trees).

