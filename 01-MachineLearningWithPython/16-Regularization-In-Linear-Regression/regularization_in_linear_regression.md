# ğŸ“˜ Regularizarea Ã®n regresie È™i clasificare

Regularizarea este o tehnicÄƒ folositÄƒ pentru a **controla complexitatea modelelor** È™i pentru a preveni **supraÃ®nvÄƒÈ›area (overfitting)**. Ea adaugÄƒ o penalizare asupra coeficienÈ›ilor modelului pentru a evita valori exagerat de mari È™i pentru a Ã®mbunÄƒtÄƒÈ›i **generalizarea** pe date noi.

---

## 1) Probleme Ã®n regresia obiÈ™nuitÄƒ

- Regresia liniarÄƒ obiÈ™nuitÄƒ ajusteazÄƒ coeficienÈ›ii pentru a minimiza **eroarea pÄƒtraticÄƒ medie (MSE)**.  
- Ãn prezenÈ›a:
  - **zgomotului**,
  - **outlierilor**,
  - sau a unui numÄƒr foarte mare de **caracteristici (features)**,  
  coeficienÈ›ii pot deveni foarte mari â†’ ceea ce duce la instabilitate È™i predicÈ›ii slabe pe date noi.

---

## 2) Regularizarea

Regularizarea rezolvÄƒ aceste probleme adÄƒugÃ¢nd un termen de **penalizare** Ã®n funcÈ›ia de cost:

### ğŸ”¹ Ridge Regression (L2 regularization)
- AdaugÄƒ penalizare proporÈ›ionalÄƒ cu **suma pÄƒtratelor coeficienÈ›ilor**.  
- FuncÈ›ia de cost devine:  
  \[
  J(\theta) = MSE + \alpha \sum \theta_i^2
  \]
- Efect: coeficienÈ›ii sunt **micÈ™oraÈ›i**, dar rÄƒmÃ¢n diferiÈ›i de zero.  
- Bun pentru situaÈ›ii cu multe variabile corelate.  

---

### ğŸ”¹ Lasso Regression (L1 regularization)
- AdaugÄƒ penalizare proporÈ›ionalÄƒ cu **suma valorilor absolute ale coeficienÈ›ilor**.  
- FuncÈ›ia de cost devine:  
  \[
  J(\theta) = MSE + \alpha \sum |\theta_i|
  \]
- Efect: unele coeficiente devin **exact zero** â†’ selecteazÄƒ automat caracteristicile importante.  
- Bun pentru **reducerea dimensionalitÄƒÈ›ii** È™i **feature selection**.  

---

### ğŸ”¹ Elastic Net
- CombinaÈ›ie Ã®ntre Ridge È™i Lasso.  
- ControleazÄƒ echilibrul dintre micÈ™orarea coeficienÈ›ilor È™i eliminarea unor variabile.  

---

## 3) Efectele regularizÄƒrii

- **CoeficienÈ›i mai mici** â†’ modelul devine mai simplu.  
- **RezistenÈ›Äƒ mai mare la outlieri** â†’ nu mai este puternic influenÈ›at de valori extreme.  
- **Generalizare mai bunÄƒ** â†’ performanÈ›Äƒ mai stabilÄƒ pe datele de test.  
- **Selectarea caracteristicilor** â†’ Lasso eliminÄƒ variabile irelevante.  

---

## 4) ComparaÈ›ie vizualÄƒ

- **Regresie liniarÄƒ simplÄƒ** â†’ se potriveÈ™te prea mult pe date, afectatÄƒ de outlieri.  
- **Ridge** â†’ linie de regresie mai robustÄƒ, nu exagereazÄƒ coeficienÈ›ii.  
- **Lasso** â†’ linie de regresie robustÄƒ + eliminÄƒ caracteristici inutile.  

---

## 5) Evaluarea modelelor de regresie

Metrici folosiÈ›i pentru compararea regresiei obiÈ™nuite, Ridge È™i Lasso:

- **Explained Variance (EV)** â†’ cÃ¢t de mult din variaÈ›ia datelor este explicatÄƒ de model.  
- **RÂ² Score** â†’ proporÈ›ia variaÈ›iei explicate.  
- **MAE (Mean Absolute Error)** â†’ eroarea medie absolutÄƒ.  
- **MSE (Mean Squared Error)** â†’ eroarea pÄƒtraticÄƒ medie.  
- **RMSE (Root Mean Squared Error)** â†’ abaterea standard a erorii.  

---

## 6) Regularizarea Ã®n regresie multivariatÄƒ (high-dimensional)

- Ãn dataseturi cu **mii de features**, regresia simplÄƒ poate fi instabilÄƒ.  
- **Ridge** â†’ pÄƒstreazÄƒ toate caracteristicile, dar le micÈ™oreazÄƒ coeficienÈ›ii.  
- **Lasso** â†’ pÄƒstreazÄƒ doar caracteristicile cu impact semnificativ, setÃ¢nd coeficienÈ›ii altora la zero.  
- **Feature selection cu Lasso** â†’ permite reducerea dimensionalitÄƒÈ›ii È™i simplificarea modelului.  

---

## 7) Concluzii

- **Linear Regression**: sensibilÄƒ la outlieri È™i la multe variabile.  
- **Ridge Regression**: bunÄƒ pentru multicoliniaritate, reduce magnitudinea coeficienÈ›ilor.  
- **Lasso Regression**: bunÄƒ pentru reducerea dimensionalitÄƒÈ›ii, eliminÄƒ caracteristici irelevante.  
- **Elastic Net**: echilibrul optim Ã®ntre Ridge È™i Lasso.  

---

âœ… **Ideea centralÄƒ**:  
Regularizarea (Ridge È™i Lasso) face modelele **mai robuste, mai generalizabile È™i mai simple**, prevenind supraÃ®nvÄƒÈ›area È™i permiÈ›Ã¢nd **selecÈ›ia automatÄƒ a caracteristicilor**.
