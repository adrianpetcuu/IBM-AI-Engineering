# ğŸ“Š Evaluating Classification Models

## 1. Train/Test Split
- **Scop**: Evaluarea performanÈ›ei modelului pe date **nevÄƒzute**.
- ÃmpÄƒrÈ›im datele Ã®n:
  - **Training set** â†’ modelul Ã®nvaÈ›Äƒ
  - **Test set** â†’ modelul este evaluat

---

## 2. Principalele Metrici de Evaluare

### âœ… Accuracy (AcurateÈ›e)
- ProporÈ›ia de predicÈ›ii corecte (TP + TN) din total.
- BunÄƒ cÃ¢nd clasele sunt **echilibrate**, dar Ã®nÈ™elÄƒtoare dacÄƒ existÄƒ **clasÄƒ rarÄƒ**.

---

### ğŸ” Confusion Matrix (Matricea de confuzie)
- AratÄƒ distribuÈ›ia predicÈ›iilor faÈ›Äƒ de realitate:
  - **TP** (True Positives) â€“ cazuri pozitive corect clasificate
  - **TN** (True Negatives) â€“ cazuri negative corect clasificate
  - **FP** (False Positives) â€“ cazuri negative clasificate ca pozitive
  - **FN** (False Negatives) â€“ cazuri pozitive clasificate ca negative

---

### ğŸ¯ Precision (Precizie)
- ProporÈ›ia predicÈ›iilor pozitive care sunt corecte.
- Formula: `TP / (TP + FP)`
- ImportantÄƒ cÃ¢nd costul **fals-pozitivelor** este mare (ex: diagnostic greÈ™it de cancer).

---

### ğŸ“Œ Recall (Sensibilitate / TPR)
- ProporÈ›ia cazurilor pozitive detectate corect.
- Formula: `TP / (TP + FN)`
- ImportantÄƒ cÃ¢nd costul **fals-negative** este mare (ex: fraudÄƒ nedetectatÄƒ).

---

### âš–ï¸ F1-Score
- Media armonicÄƒ dintre **Precision** È™i **Recall**.
- UtilÄƒ cÃ¢nd existÄƒ **clase dezechilibrate**.
- Formula: `2 * (Precision * Recall) / (Precision + Recall)`

---

## 3. Alte metrici utile
- **Specificitate (TNR)**: cÃ¢t de bine detecteazÄƒ modelul cazurile negative â†’ `TN / (TN + FP)`
- **ROC Curve & AUC**:
  - ROC = raport Ã®ntre **TPR È™i FPR**
  - AUC = aria de sub curbÄƒ (0.5 = aleatoriu, 1.0 = perfect)

---

## 4. Exemple practice: KNN È™i SVM
### ğŸ”¹ KNN (K-Nearest Neighbors)
- Algoritm bazat pe vecini.
- Sensibil la **scalarea datelor** È™i la **zgomot**.

### ğŸ”¹ SVM (Support Vector Machine)
- CreeazÄƒ un **hiperplan optim** pentru separarea claselor.
- Kernel `linear` sau `rbf` pentru date mai complexe.

---

## 5. Interpretarea Rezultatelor
- EvaluÄƒm **atÃ¢t pe train cÃ¢t È™i pe test set** pentru a depista:
  - **Overfitting** (scor mare pe train, slab pe test)
  - **Underfitting** (scor slab pe ambele)

- **Confusion Matrix + Classification Report** â†’ aratÄƒ distribuÈ›ia precisÄƒ a erorilor.

---

## 6. Concluzii
- Nu ne bazÄƒm doar pe **accuracy**.
- Alegerea metricii depinde de **context**:
  - Cost ridicat al FP â†’ **Precision**
  - Cost ridicat al FN â†’ **Recall**
  - Clase dezechilibrate â†’ **F1-score**, **ROC-AUC**
- VizualizÄƒrile (heatmap, ROC curve) ajutÄƒ la interpretarea corectÄƒ a performanÈ›ei.
