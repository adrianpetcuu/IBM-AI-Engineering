# ğŸ“˜ Gradient Descent & Backpropagation

## ğŸ”¹ Gradient Descent (Algoritmul de coborÃ¢re a gradientului)

### âœ… Ce este?
- **Gradient Descent** este un **algoritm de optimizare iterativ** folosit pentru a gÄƒsi **minimum-ul unei funcÈ›ii de cost** (loss function).
- Este folosit pentru **antrenarea reÈ›elelor neuronale**, prin ajustarea **greutÄƒÈ›ilor (weights)** È™i **biasurilor** pentru a reduce eroarea dintre predicÈ›ie È™i adevÄƒr.

---

### âœ… Cum funcÈ›ioneazÄƒ?
1. **Se calculeazÄƒ funcÈ›ia de cost (J)** â†’ mÄƒsoarÄƒ diferenÈ›a Ã®ntre predicÈ›ii È™i valorile reale.  
2. **Se calculeazÄƒ gradientul (derivata funcÈ›iei de cost faÈ›Äƒ de weights & bias)**.  
   - Gradientul aratÄƒ **direcÈ›ia creÈ™terii erorii**.  
3. **Actualizare parametri (weights & bias)** Ã®n direcÈ›ia opusÄƒ gradientului:  
   \[
   w := w - \eta \cdot \frac{\partial J}{\partial w}
   \]
   - `Î·` (eta) = **rata de Ã®nvÄƒÈ›are** (learning rate).  
   - `âˆ‚J/âˆ‚w` = gradientul erorii faÈ›Äƒ de greutate.  

---

### âœ… Tipuri de Gradient Descent
- **Batch Gradient Descent** â†’ foloseÈ™te Ã®ntreg setul de date pentru fiecare actualizare.  
- **Stochastic Gradient Descent (SGD)** â†’ actualizeazÄƒ parametrii dupÄƒ fiecare eÈ™antion (mai rapid, dar zgomotos).  
- **Mini-batch Gradient Descent** â†’ compromis Ã®ntre cele douÄƒ, foloseÈ™te loturi mici de date (foarte des utilizat Ã®n deep learning).  

---

### âœ… Probleme & SoluÈ›ii
- **Learning rate prea mare** â†’ sare peste minim.  
- **Learning rate prea mic** â†’ converge foarte lent.  
- **Local minima / saddle points** â†’ reÈ›eaua poate rÄƒmÃ¢ne blocatÄƒ.  
- Se folosesc **optimizatori avansaÈ›i**: Adam, RMSProp, AdaGrad.  

---

---

## ğŸ”¹ Backpropagation (Algoritmul de propagare Ã®napoi)

### âœ… Ce este?
- **Backpropagation** = algoritmul prin care reÈ›eaua neuronalÄƒ **Ã®nvaÈ›Äƒ ajustÃ¢nd greutÄƒÈ›ile** pe baza erorii.  
- FuncÈ›ioneazÄƒ Ã®mpreunÄƒ cu **Gradient Descent**.  
- EsenÈ›a: **error â†’ backward â†’ update weights**.  

---

### âœ… Cum funcÈ›ioneazÄƒ?
1. **Forward Propagation**:
   - Se calculeazÄƒ predicÈ›ia (output-ul reÈ›elei).
   - Se comparÄƒ cu **valoarea realÄƒ** â†’ se obÈ›ine **eroarea (loss)**.  

2. **Backward Propagation**:
   - Se aplicÄƒ **teorema lanÈ›ului (chain rule)** pentru a calcula derivata erorii faÈ›Äƒ de fiecare greutate È™i bias.  
   - Se obÈ›in **gradientele pentru fiecare strat** al reÈ›elei.  

   Exemplu simplu pentru un neuron:  
   \[
   \frac{\partial J}{\partial w} = \frac{\partial J}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}
   \]

   - `a` = activarea (ieÈ™irea neuronului dupÄƒ funcÈ›ia de activare).  
   - `z` = suma ponderatÄƒ (wÂ·x + b).  

3. **Actualizare parametri**:
   - Folosind regula Gradient Descent, se actualizeazÄƒ greutÄƒÈ›ile È™i biasurile.  

---

### âœ… De ce e important?
- Face posibil antrenamentul reÈ›elelor neuronale **complexe cu multe straturi** (Deep Learning).  
- FÄƒrÄƒ backpropagation, nu am putea ajusta greutÄƒÈ›ile eficient.  
- Este cheia succesului Ã®n recunoaÈ™tere imagini, NLP, audio etc.  

---

### âœ… Probleme frecvente
- **Vanishing Gradient Problem** (mai ales cu funcÈ›ii ca Sigmoid/tanh):  
  - gradientele devin extrem de mici â†’ straturile iniÈ›iale Ã®nvaÈ›Äƒ foarte greu.  
  - SoluÈ›ii: ReLU, Leaky ReLU, batch normalization, optimizatori avansaÈ›i.  

- **Exploding Gradients** â†’ valorile devin foarte mari â†’ instabilitate.  
  - SoluÈ›ii: gradient clipping, optimizatori avansaÈ›i.  

---

## ğŸ“ Concluzie
- **Gradient Descent** = metoda de optimizare prin care gÄƒsim greutÄƒÈ›ile potrivite pentru a minimiza eroarea.  
- **Backpropagation** = algoritmul care calculeazÄƒ gradientele erorii pentru fiecare parametru, astfel Ã®ncÃ¢t Gradient Descent sÄƒ le poatÄƒ actualiza.  

ğŸ‘‰ Pe scurt:  
1. **Forward Propagation** â†’ facem predicÈ›ia  
2. **CalculÄƒm eroarea**  
3. **Backpropagation** â†’ calculÄƒm gradientele  
4. **Gradient Descent** â†’ actualizÄƒm greutÄƒÈ›ile  
