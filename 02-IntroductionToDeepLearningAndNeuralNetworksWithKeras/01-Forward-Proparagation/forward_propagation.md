# ğŸ“˜ Introduction to Deep Learning, Neurons and Neural Networks

## ğŸ”¹ 1. Introduction to Deep Learning
- Deep Learning este o ramurÄƒ a Machine Learning bazatÄƒ pe **reÈ›ele neuronale artificiale cu multe straturi** (Deep Neural Networks).
- Inspirat din modul Ã®n care funcÈ›ioneazÄƒ creierul uman.
- AplicaÈ›ii:
  - recunoaÈ™tere imagini
  - procesarea limbajului natural (NLP)
  - recunoaÈ™tere vocalÄƒ
  - sisteme autonome (maÈ™ini self-driving)

---

## ğŸ”¹ 2. Neurons and Neural Networks
- **Neuron artificial** = unitate de calcul care primeÈ™te intrÄƒri, le combinÄƒ cu **greutÄƒÈ›i (weights)** È™i **bias**, apoi trece rezultatul printr-o **funcÈ›ie de activare**.
- Structura unei reÈ›ele neuronale:
  - **Input layer** (strat de intrare)
  - **Hidden layers** (straturi ascunse)
  - **Output layer** (strat de ieÈ™ire)
- ReÈ›elele neuronale permit modelarea **relaÈ›iilor complexe È™i neliniare**.

---

## ğŸ”¹ 3. Artificial Neural Networks (ANN)
- O reÈ›ea neuronalÄƒ artificialÄƒ este formatÄƒ din **neuroni conectaÈ›i Ã®ntre straturi**.
- **Weights È™i bias** sunt parametrii ajustaÈ›i Ã®n timpul antrenÄƒrii.
- Procesul de Ã®nvÄƒÈ›are:
  1. IniÈ›ializare greutÄƒÈ›i È™i bias la valori mici random.
  2. Calculul ieÈ™irii reÈ›elei (**Forward Propagation**).
  3. Calculul erorii faÈ›Äƒ de valorile reale.
  4. Corectarea greutÄƒÈ›ilor prin **Backpropagation + Gradient Descent**.

---

## ğŸ”¹ 4. Forward Propagation (Detaliat)
Forward Propagation este procesul prin care datele de intrare sunt transmise prin reÈ›ea, strat cu strat, pÃ¢nÄƒ la ieÈ™irea finalÄƒ.  

### ğŸ”¸ PaÈ™ii principali:
1. **IntrÄƒrile sunt primite Ã®n stratul de input** (ex: imagine, text, date numerice).
2. **Calculul sumei ponderate pentru fiecare neuron**:
   \[
   z = sum (w_i * x_i) + b
   \]
   - `x_i` = valorile de intrare  
   - `w_i` = greutÄƒÈ›ile asociate conexiunilor  
   - `b` = bias (constanta care deplaseazÄƒ funcÈ›ia)

3. **Aplicarea funcÈ›iei de activare**:
   - Sigmoid: produce valori Ã®ntre 0 È™i 1 â†’ utilÄƒ pentru probabilitÄƒÈ›i.
   - tanh: produce valori Ã®ntre -1 È™i 1 â†’ mai centratÄƒ Ã®n jurul originii.
   - ReLU: pÄƒstreazÄƒ doar valorile pozitive â†’ accelereazÄƒ Ã®nvÄƒÈ›area.
   - Softmax: folositÄƒ Ã®n stratul de ieÈ™ire pentru clasificare multi-clasÄƒ.

   \[
   a = f(z)
   \]
   unde `f` este funcÈ›ia de activare.

4. **Transmiterea rezultatelor la stratul urmÄƒtor**:
   - IeÈ™irea fiecÄƒrui neuron devine **intrarea** neuronilor din stratul urmÄƒtor.
   - Acest proces se repetÄƒ pentru toate **hidden layers**.

5. **PredicÈ›ia finalÄƒ la Output Layer**:
   - Ãn clasificare binarÄƒ â†’ Sigmoid.  
   - Ãn clasificare multi-clasÄƒ â†’ Softmax.  
   - Ãn regresie â†’ funcÈ›ie liniarÄƒ.

---

### ğŸ”¸ Exemplu simplificat:
- Input: [x1, x2]  
- Stratul ascuns calculeazÄƒ:
  \[
  z_1 = w_1 * x_1 + w_2 * x_2 + b
  \]
  \[
  a_1 = f(z_1)
  \]
- Output layer:
  \[
  \hat{y} = f(w * a_1 + b)
  \]

---

### ğŸ”¸ Rolul Forward Propagation:
âœ… Permite **calculul predicÈ›iei** pentru orice intrare.  
âœ… Este prima etapÄƒ Ã®nainte de **calculul erorii** È™i **Backpropagation**.  
âœ… Introduce **non-liniaritate** prin funcÈ›iile de activare â†’ esenÈ›ial pentru Ã®nvÄƒÈ›area relaÈ›iilor complexe.  

---

## âœ… Concluzie
- Deep Learning permite antrenarea de modele complexe inspirate din biologia creierului.
- Neuronii artificiali folosesc **weights + bias + funcÈ›ii de activare**.
- Artificial Neural Networks se antreneazÄƒ prin:
  - **Forward Propagation** â†’ calculul predicÈ›iei
  - **Backpropagation + Gradient Descent** â†’ ajustarea greutÄƒÈ›ilor.
