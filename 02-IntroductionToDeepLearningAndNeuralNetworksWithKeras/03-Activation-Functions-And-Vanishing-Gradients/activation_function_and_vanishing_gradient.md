# ğŸ“˜ Vanishing Gradient & Activation Functions

## ğŸ”¹ Vanishing Gradient

### âœ… Ce este?
- **Problema vanishing gradient** apare atunci cÃ¢nd **valorile gradientului devin extrem de mici** Ã®n timpul backpropagation.  
- Aceasta face ca **straturile iniÈ›iale ale reÈ›elei neuronale sÄƒ Ã®nveÈ›e foarte greu sau aproape deloc**.  

---

### âœ… Cum apare?
1. Ãn reÈ›ele adÃ¢nci (cu multe straturi), la fiecare pas de backpropagation gradientul este **Ã®nmulÈ›it cu valori < 1**.  
2. DupÄƒ mai multe Ã®nmulÈ›iri, gradientul devine **aproape zero**.  
3. Astfel, **actualizarea greutÄƒÈ›ilor din straturile timpurii (aproape de input) Ã®ncetineÈ™te** drastic.  

---

### âœ… Exemple de funcÈ›ii afectate
- **Sigmoid** â†’ gradient mic pentru valori mari pozitive sau negative.  
- **Tanh** â†’ similar, gradient mic pentru valori extreme.  

---

### âœ… ConsecinÈ›e
- Straturile din faÈ›Äƒ Ã®nvaÈ›Äƒ foarte Ã®ncet.  
- Modelul converge greu È™i poate avea o **acurateÈ›e scÄƒzutÄƒ**.  

---

### âœ… SoluÈ›ii
- Folosirea altor funcÈ›ii de activare:
  - **ReLU (Rectified Linear Unit)** â†’ menÈ›ine gradient constant pentru valori pozitive.  
  - **Leaky ReLU** â†’ reduce problema neuronilor â€morÈ›iâ€.  
- **Batch Normalization** â†’ normalizeazÄƒ activÄƒrile È™i reduce scÄƒderea gradientului.  
- Optimizatori mai buni: **Adam, RMSProp**.  

---

---

## ğŸ”¹ Activation Functions (FuncÈ›ii de activare)

### âœ… Ce sunt?
- FuncÈ›iile de activare introduc **non-liniaritate** Ã®n reÈ›ele neuronale.  
- Ele decid dacÄƒ un neuron trebuie sÄƒ fie activat sau nu, permiÈ›Ã¢nd reÈ›elelor sÄƒ Ã®nveÈ›e **relaÈ›ii complexe**.  

---

### âœ… Tipuri principale

1. **Sigmoid Function**
   \[
   \sigma(x) = \frac{1}{1 + e^{-x}}
   \]
   - TransformÄƒ valorile Ã®ntre **0 È™i 1**.  
   - UtilÄƒ pentru probleme de **clasificare binarÄƒ**.  
   - âŒ Dezavantaj: suferÄƒ de **vanishing gradient**.

---

2. **Hyperbolic Tangent (tanh)**
   \[
   \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
   \]
   - Output Ã®ntre **-1 È™i 1**.  
   - Mai bunÄƒ decÃ¢t sigmoid (simetricÄƒ faÈ›Äƒ de origine).  
   - âŒ TotuÈ™i, È™i ea poate cauza **vanishing gradient**.  

---

3. **ReLU (Rectified Linear Unit)**
   \[
   f(x) = \max(0, x)
   \]
   - Output: **0 pentru valori negative**, **x pentru valori pozitive**.  
   - âœ… Avantaj: **evitÄƒ vanishing gradient** pentru valori pozitive.  
   - âŒ ProblemÄƒ: â€neuroni morÈ›iâ€ pentru valori negative.  

---

4. **Leaky ReLU**
   \[
   f(x) = \begin{cases} x & x > 0 \\ \alpha x & x \leq 0 \end{cases}
   \]
   - MicÄƒ pantÄƒ (Î±) pentru valori negative.  
   - âœ… EvitÄƒ problema neuronilor morÈ›i.  

---

5. **Softmax**
   \[
   \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
   \]
   - NormalizeazÄƒ output-ul astfel Ã®ncÃ¢t suma sÄƒ fie **1**.  
   - âœ… UtilizatÄƒ Ã®n **stratul de ieÈ™ire** pentru probleme de **clasificare multi-clasÄƒ**.  

---

### âœ… Concluzie
- **Sigmoid / Tanh** â†’ simple, dar predispun la vanishing gradient.  
- **ReLU / Leaky ReLU** â†’ preferate pentru straturile ascunse (hidden layers).  
- **Softmax** â†’ obligatorie pentru stratul de ieÈ™ire Ã®n clasificare multi-class.  
